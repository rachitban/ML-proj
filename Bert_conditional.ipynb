{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_babble.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jhTd38PTeJL",
        "colab_type": "code",
        "outputId": "9778fc35-010c-4864-eeac-8dd9fb0e58d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "!pip3 install pytorch_pretrained_bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.11.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (1.12.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2W-F7YrTgK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6BpUNlBTjJo",
        "colab_type": "code",
        "outputId": "e207e040-c696-4077-d803-091a939e4ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_version = 'bert-base-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_version)\n",
        "model.eval()\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
        "\n",
        "def tokenize_batch(batch):\n",
        "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
        "\n",
        "def untokenize_batch(batch):\n",
        "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
        "\n",
        "def detokenize(sent):\n",
        "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(sent):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "\n",
        "CLS = '[CLS]'\n",
        "SEP = '[SEP]'\n",
        "MASK = '[MASK]'\n",
        "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
        "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:04<00:00, 85006790.88B/s]\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 6012662.50B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TPa6BsRTulK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
        "    \"\"\" Generate a word from from out[gen_idx]\n",
        "    \n",
        "    args:\n",
        "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
        "        - gen_idx (int): location for which to generate for\n",
        "        - top_k (int): if >0, only sample from the top k most probable words\n",
        "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
        "    \"\"\"\n",
        "    logits = out[:, gen_idx]\n",
        "    if temperature is not None:\n",
        "        logits = logits / temperature\n",
        "    if top_k > 0:\n",
        "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
        "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
        "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
        "    elif sample:\n",
        "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
        "        idx = dist.sample().squeeze(-1)\n",
        "    else:\n",
        "        idx = torch.argmax(logits, dim=-1)\n",
        "    return idx.tolist() if return_list else idx\n",
        "  \n",
        "  \n",
        "def get_init_text(word,r,seed_text, max_len, batch_size = 1, rand_init=False):\n",
        "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
        "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
        "    \n",
        "\n",
        "    for x in batch:\n",
        "      x[r]=word\n",
        "    #if rand_init:\n",
        "    #    for ii in range(max_len):\n",
        "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
        "\n",
        "    return tokenize_batch(batch)\n",
        "\n",
        "def printer(sent, should_detokenize=True):\n",
        "    if should_detokenize:\n",
        "        sent = detokenize(sent)[1:-1]\n",
        "    print(\" \".join(sent))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VzY3scBYc3H",
        "colab_type": "text"
      },
      "source": [
        "This is the meat of the algorithm. The general idea is\n",
        "1. start from all masks\n",
        "2. repeatedly pick a location, mask the token at that location, and generate from the probability distribution given by BERT\n",
        "3. stop when converged or tired of waiting\n",
        "\n",
        "We consider three \"modes\" of generating:\n",
        "- generate a single token for a position chosen uniformly at random for a chosen number of time steps\n",
        "- generate in sequential order (L->R), one token at a time\n",
        "- generate for all positions at once for a chosen number of time steps\n",
        "\n",
        "The `generate` function wraps and batches these three generation modes. In practice, we find that the first leads to the most fluent samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BR0JVmlTvEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generation modes as functions\n",
        "import math\n",
        "import time\n",
        "\n",
        "def parallel_sequential_generation(word,seed_text, batch_size=10, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
        "                                   cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for one random position at a timestep\n",
        "    \n",
        "    args:\n",
        "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
        "    \"\"\"\n",
        "    r = np.random.randint(0, max_len)\n",
        "    \n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(word,r,seed_text, max_len, batch_size)\n",
        "    \n",
        "    #print(batch)\n",
        "\n",
        "    for ii in range(max_iter):\n",
        "        \n",
        "        kk = np.random.randint(0, max_len)\n",
        "        \n",
        "        if(kk+seed_len==r):\n",
        "            continue\n",
        "\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = mask_id\n",
        "\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        topk = top_k if (ii >= burnin) else 0\n",
        "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = idxs[jj]\n",
        "\n",
        "        if verbose and np.mod(ii+1, print_every) == 0:\n",
        "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
        "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
        "            print(\"iter\", ii+1, \" \".join(for_print))\n",
        "            \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "def parallel_generation(word,seed_text, batch_size=10, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
        "                        cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for all positions at each time step \"\"\"\n",
        "    \n",
        "    r = np.random.randint(0, max_len)\n",
        "    \n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(word,r,seed_text, max_len, batch_size)\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        for kk in range(max_len):\n",
        "            if(kk+seed_len==r):\n",
        "              continue\n",
        "            idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, temperature=temperature, sample=sample)\n",
        "            for jj in range(batch_size):\n",
        "                batch[jj][seed_len+kk] = idxs[jj]\n",
        "            \n",
        "        if verbose and np.mod(ii, print_every) == 0:\n",
        "            print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
        "    \n",
        "    return untokenize_batch(batch)\n",
        "            \n",
        "def sequential_generation(word,seed_text, batch_size=10, max_len=15, leed_out_len=15, \n",
        "                          top_k=0, temperature=None, sample=True, cuda=False):\n",
        "    \"\"\" Generate one word at a time, in L->R order \"\"\"\n",
        "    r = np.random.randint(0, max_len)\n",
        "    \n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(word,r,seed_text, max_len, batch_size)\n",
        "    \n",
        "    for ii in range(max_len):\n",
        "\n",
        "        if(ii+seed_len==r):\n",
        "          continue\n",
        "\n",
        "        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        out = model(inp)\n",
        "        idxs = generate_step(out, gen_idx=seed_len+ii, top_k=top_k, temperature=temperature, sample=sample)\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+ii] = idxs[jj]\n",
        "        \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "\n",
        "def generate(word,n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
        "             generation_mode=\"parallel-sequential\",\n",
        "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
        "             cuda=False, print_every=1):\n",
        "    # main generation function to call\n",
        "    sentences = []\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    start_time = time.time()\n",
        "    for batch_n in range(n_batches):\n",
        "        if generation_mode == \"parallel-sequential\":\n",
        "            batch = parallel_sequential_generation(word,seed_text, batch_size=batch_size, max_len=max_len, top_k=top_k,\n",
        "                                                   temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
        "                                                   cuda=cuda, verbose=False)\n",
        "        elif generation_mode == \"sequential\":\n",
        "            batch = sequential_generation(word,seed_text, batch_size=batch_size, max_len=max_len, top_k=top_k, \n",
        "                                          temperature=temperature, leed_out_len=leed_out_len, sample=sample,\n",
        "                                          cuda=cuda)\n",
        "        elif generation_mode == \"parallel\":\n",
        "            batch = parallel_generation(word,seed_text, batch_size=batch_size,\n",
        "                                        max_len=max_len, top_k=top_k, temperature=temperature, \n",
        "                                        sample=sample, max_iter=max_iter, \n",
        "                                        cuda=cuda, verbose=False)\n",
        "        \n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "        \n",
        "        sentences += batch\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKLZ5U71XGv1",
        "colab_type": "text"
      },
      "source": [
        "Let's call the actual generation function! We'll use the following settings\n",
        "- max_len (40): length of sequence to generate\n",
        "- top_k (100): at each step, sample from the top_k most likely words\n",
        "- temperature (1.0): smoothing parameter for the next word distribution. Higher means more like uniform; lower means more peaky\n",
        "- burnin (250): for non-sequential generation, for the first burnin steps, sample from the entire next word distribution, instead of top_k\n",
        "- max_iter (500): number of iterations to run for\n",
        "- seed_text ([\"CLS\"]): prefix to generate for. We found it crucial to start with the CLS token; you can try adding to it "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0So4YGiT5V_",
        "colab_type": "code",
        "outputId": "8ca58e41-6fc3-41f5-a140-dd3afbce55ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "n_samples = 4\n",
        "batch_size = 2\n",
        "max_len = 15\n",
        "top_k = 100\n",
        "temperature = 1.0\n",
        "generation_mode = \"parallel-sequential\"\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "#print(tokenizer.convert_tokens_to_ids(['happy']))\n",
        "\n",
        "# Choose the prefix context\n",
        "seed_text = \"[CLS]\".split()\n",
        "\n",
        "word=input(\"Enter the conditional word \")\n",
        "\n",
        "bert_sents = generate(word,n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                      generation_mode=generation_mode,\n",
        "                      sample=sample, top_k=top_k, temperature=temperature, burnin=burnin, max_iter=max_iter,\n",
        "                      cuda=cuda)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the conditional word confused\n",
            "Finished batch 1 in 7.747s\n",
            "Finished batch 2 in 7.446s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MychO6GwVKVI",
        "colab_type": "code",
        "outputId": "6640e2b1-128b-4dfe-c9cb-8186a0b37423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "for sent in bert_sents:\n",
        "  printer(sent, should_detokenize=True)\n",
        "  print(\"\\n\")\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "we can be there for a long time . \" everybody made confused faces .\n",
            "\n",
            "\n",
            "the torches suddenly burst into flame , flooding the main house with confused light .\n",
            "\n",
            "\n",
            "his relationship with the man who had killed tony harper was confused and vulnerable .\n",
            "\n",
            "\n",
            "seeing something in her brown eyes , i went straight from confused to happy .\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGNnNJf-Vm4d",
        "colab_type": "text"
      },
      "source": [
        "# **Evaluation**\n",
        "\n",
        "Evaluation methods for unconditional generation aren't perfect. We'll measure the diversity of our generated samples via self-BLEU: we compute corpus BLEU where for each generated sentence, we compute BLEU treating the other sentences as references. We also compute the percentage of $n$-grams that are unique among the generations. We try some other strategies, including comparing to outside models, in our report, and you can see some of the code for that [here](https://github.com/kyunghyuncho/bert-gen/blob/master/bert-babble.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqhCiFSNVzTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from nltk.translate import bleu_score as bleu\n",
        "\n",
        "def self_bleu(sents):\n",
        "    return bleu.corpus_bleu([[s for (j, s) in enumerate(sents) if j != i] for i in range(len(sents))], sents)\n",
        "\n",
        "def get_ngram_counts(sents, max_n=4):\n",
        "    size2count = {}\n",
        "    for i in range(1, max_n + 1):\n",
        "        size2count[i] = Counter([n for sent in sents for n in ngrams(sent, i)])\n",
        "    return size2count\n",
        "        \n",
        "def self_unique_ngrams(preds, max_n=4):\n",
        "    # get # of pred ngrams with count 1\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    for i in range(1, max_n + 1):\n",
        "        n_unique = len([k for k, v in pred_ngrams[i].items() if v == 1])\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        pct_unique[i] = n_unique / total\n",
        "    return pct_unique"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2Lp5HgcW4of",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_n = 4\n",
        "print(\"BERT %s self-BLEU: %.2f\" % (model_version, 100 * self_bleu(bert_sents)))\n",
        "\n",
        "pct_uniques = self_unique_ngrams(bert_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT %s unique %d-grams relative to self: %.2f\" % (model_version, i, 100 * pct_uniques[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}